"""Kubernetes Agent module.

This module defines a Kubernetes agent that integrates multiple tools,
handles shell commands, and interacts with OpenAI's chat models to
process user requests. It leverages langchain's agent executor framework
to facilitate agent-like behaviors.
"""

from typing import List, Dict, Optional
from langchain.agents import AgentExecutor
from langchain.agents.format_scratchpad.openai_tools import (
    format_to_openai_tool_messages,
)
from langchain.pydantic_v1 import BaseModel
from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.runnables import (
    RunnableParallel,
    RunnablePassthrough,
    RunnableLambda,
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools import ShellTool, DuckDuckGoSearchRun
from langchain_openai import ChatOpenAI

from . import settings, prompts


#: Initialize a ShellTool and modify its description to handle arg placeholders
shell_tool = ShellTool()
shell_tool.description = shell_tool.description + f"args {shell_tool.args}".replace(
    "{", "{{"
).replace("}", "}}")
#: Initialize a DuckDuckGo search tool
search_tool = DuckDuckGoSearchRun()

#: List of tools for the agent to use, including the shell tool and search tool
tools = [shell_tool, search_tool]
#: Initialize the OpenAI chat model with settings from the configuration
llm = ChatOpenAI(
    model=settings.OPENAI_MODEL, temperature=settings.OPENAI_MODEL_TEMP, streaming=True
)
llm = llm.bind_tools(tools)

#: Define the prompt template for the conversation, including placeholders
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", prompts.SYSTEM_PROMPT),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)


class ChatRequest(BaseModel):
    """Data model representing a chat request.

    This object encapsulates the input message and optionally includes
    chat history for context.

    .. autoattribute:: input

       The user's input message.

    .. autoattribute:: chat_history

       A list of dictionaries containing previous messages (both human and AI).
    """

    input: str
    chat_history: Optional[List[Dict[str, str]]]


class Output(BaseModel):
    """Data model representing the output response from the agent.

    .. autoattribute:: output

       The response generated by the agent.
    """
    output: str


def serialize_history(request: ChatRequest):
    """Serialize chat history into HumanMessage and AIMessage objects.

    This function converts a list of dictionaries representing the
    chat history into a list of langchain_core messages (HumanMessage
    for human messages, AIMessage for AI responses).

    :param ChatRequest request:
       The incoming request object that includes the chat history.

    :return:
       A `list` of converted HumanMessage and AIMessage objects.
    """
    chat_history = request["chat_history"] or []
    converted_chat_history = []
    for message in chat_history:
        if message.get("human") is not None:
            converted_chat_history.append(HumanMessage(content=message["human"]))
        if message.get("ai") is not None:
            converted_chat_history.append(AIMessage(content=message["ai"]))
    return converted_chat_history

#: Define the agent behavior using Runnable components, including shell commands
agent = (
    RunnableParallel(
        input=RunnablePassthrough(),
        chat_history=RunnableLambda(serialize_history),
        agent_scratchpad=lambda x: format_to_openai_tool_messages(
            x["intermediate_steps"]
        ),
        helm_list=lambda _: shell_tool.run("helm list --all-namespaces"),
        crds=lambda _: shell_tool.run("kubectl get crds"),
        kubectl_basic_resources=lambda _: shell_tool.run(
            "kubectl get deployments,statefulset,daemonsets,services,ingresses --all-namespaces"
        ),
    )
    | prompt
    | llm
    | OpenAIToolsAgentOutputParser()
)

#: Initialize the AgentExecutor with the agent, tools, and type validation
executor = AgentExecutor(agent=agent, tools=tools, verbose=True).with_types(
    input_type=ChatRequest, output_type=Output
)
